# -*- coding: utf-8 -*-
"""Risk Analysis Using Roberta.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10_WrKfjViSPY2LyF6MoAev7ahoOlQWtW

# Task
Explain the step-by-step workflow for training a RoBERTa model for risk estimation using provided training data in a CSV format. The output should be an ordinal scale (1=Very Low Risk, 2=Low Risk, 3=Medium Risk, 4=High Risk, 5=Very High Risk). Do not include data augmentation in the workflow.

## Load data

### Subtask:
Load the training data from the provided CSV file into a pandas DataFrame.

**Reasoning**:
Import the pandas library and load the CSV data into a DataFrame.
"""

import pandas as pd

# Load data from Excel file
try:
    df = pd.read_excel("Combined_Training_Data_Final.xlsx")
    print("Data loaded successfully!")
    print("First 5 rows of the DataFrame:")
    display(df.head())
    print("\nDataFrame Info:")
    display(df.info())
except FileNotFoundError:
    print("Error: Combined_Training_Data_Final.xlsx not found. Please upload the file.")
except Exception as e:
    print(f"An error occurred while reading the Excel file: {e}")

# Ensure the 'Requirement_Text' and 'Risk' columns are present
required_columns = ['Requirement_Text', 'Risk']
if not all(col in df.columns for col in required_columns):
    raise ValueError(f"The loaded data must contain the following columns: {required_columns}")

# Drop rows where 'Risk' is missing
df_cleaned = df.dropna(subset=['Risk']).copy()

# --- Add this section to check data distribution ---
print("Risk level distribution before training:")
print(df_cleaned['Risk'].value_counts())
print("-" * 30)

# Prepare texts and labels
texts = df_cleaned['Requirement_Text'].astype(str).tolist()

# Capture the categories before encoding
risk_categories_mapping = df_cleaned['Risk'].astype('category').cat.categories.tolist()
labels = df_cleaned['Risk'].astype('category').cat.codes.tolist()  # Encode risk levels as integers

# --- Add this section to verify label mapping consistency ---
print("Encoded label to original category mapping used for training:")
for code, category in enumerate(risk_categories_mapping):
    print(f"  Code {code}: {category}")
print("-" * 30)

print(f"Number of data points after cleaning: {len(df_cleaned)}")
print(f"Number of texts: {len(texts)}")
print(f"Number of labels: {len(labels)}")

from sklearn.model_selection import train_test_split

# Split data
# Use stratify=labels to maintain class distribution in train and validation sets
train_texts, val_texts, train_labels, val_labels = train_test_split(
    texts, labels, test_size=0.2, random_state=42, stratify=labels
)

print(f"Number of training samples: {len(train_texts)}")
print(f"Number of validation samples: {len(val_texts)}")
print(f"Training labels distribution: {pd.Series(train_labels).value_counts(normalize=True)}")
print(f"Validation labels distribution: {pd.Series(val_labels).value_counts(normalize=True)}")

from transformers import RobertaTokenizer
from torch.utils.data import Dataset
import torch

# Tokenizer
tokenizer = RobertaTokenizer.from_pretrained('roberta-base')

# Custom Dataset
class RequirementsDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_length=128):
        # Tokenizer can handle lists of strings
        print("Tokenizing data...") # Add print to see tokenization progress
        # Added return_token_type_ids=False as RoBERTa doesn't use them and it can cause issues
        self.encodings = tokenizer(texts, truncation=True, padding='max_length', max_length=max_length, return_tensors='pt', return_token_type_ids=False)
        # Move encodings to CPU if they are on GPU after tokenization (depends on tokenizer version/settings)
        # Ensure data is on CPU before creating Dataset
        self.encodings = {k: v.cpu() for k, v in self.encodings.items()}
        # Convert labels to a torch tensor
        self.labels = torch.tensor(labels, dtype=torch.long) # Ensure labels are long type
        print("Tokenization finished.") # Add print

    def __getitem__(self, idx):
        # item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        # Use .clone() or just indexing if self.encodings is already a tensor
        # .detach() is often not necessary when simply indexing
        item = {key: val[idx].clone() for key, val in self.encodings.items()}
        item['labels'] = self.labels[idx].clone() # Labels are already a tensor
        return item

    def __len__(self):
        return len(self.labels)

# Create datasets with the split data
train_dataset = RequirementsDataset(train_texts, train_labels, tokenizer)
val_dataset = RequirementsDataset(val_texts, val_labels, tokenizer)

print("Training and validation datasets created.")

from transformers import RobertaForSequenceClassification, Trainer, TrainingArguments

# Model
# num_labels was determined in a previous cell from len(risk_categories_mapping)
num_labels = len(risk_categories_mapping)
model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=num_labels)

# Training arguments
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=5, # Increased epochs slightly for better training
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    eval_strategy='epoch',
    save_strategy='epoch',
    logging_dir='./logs',
    logging_steps=10,
    load_best_model_at_end=True,
    report_to=None, # Disable wandb reporting if not needed
    # Consider adding a metric like accuracy if you need it for evaluation
    # compute_metrics=lambda p: {"accuracy": (p.predictions.argmax(-1) == p.label_ids).mean()}
)

# Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    # Pass compute_metrics if you added it to TrainingArguments
    # compute_metrics=lambda p: {"accuracy": (p.predictions.argmax(-1) == p.label_ids).mean()}
)

print("Model, Training Arguments, and Trainer initialized.")

# Train
print("Starting training...")
trainer.train()
print("Training finished.")

"""## Evaluate the model
Evaluate the trained model on the validation dataset to assess its performance.
"""

# Evaluate the model
print("Evaluating the model...")
eval_results = trainer.evaluate()
print(f"Evaluation results: {eval_results}")

"""## Risk estimation from requirement statement
Use the trained model to estimate the risk of new requirement statements.
"""

# Example usage of the estimate_risk function
print("\nEstimating risk for example sentences after training:")
print(f"Requirement: 'The system must allow users to reset their passwords via email.'")
print(f"Estimated Risk: {estimate_risk('The system must allow users to reset their passwords via email.', model, tokenizer, risk_categories_mapping)}")
print("-" * 30)

print(f"Requirement: 'The system shall use two factor authentication for login.'")
print(f"Estimated Risk: {estimate_risk('The system shall use two factor authentication for login.', model, tokenizer, risk_categories_mapping)}")
print("-" * 30)

print(f"Requirement: 'User data shall be encrypted at rest and in transit.'")
print(f"Estimated Risk: {estimate_risk('User data shall be encrypted at rest and in transit.', model, tokenizer, risk_categories_mapping)}")
print("-" * 30)

print(f"Requirement: 'The system will have a simple user interface.'")
print(f"Estimated Risk: {estimate_risk('The system will have a simple user interface.', model, tokenizer, risk_categories_mapping)}")
print("-" * 30)

# Requirement 1: Existing (Healthcare, Non-Functional, RQ-04-07)
print(f"Requirement: 'The system shall support two-factor authentication for user logins.'")
print(f"Estimated Risk: {estimate_risk('The system shall support two-factor authentication for user logins.', model, tokenizer, risk_categories_mapping)}")
print("-" * 30)
# Actual Risk: 5 (from dataset, RQ-04-07, high due to security sensitivity)

# Requirement 2: Existing (Logistics, Functional, RQ-05-14)
print(f"Requirement: 'The system shall allow customers to upload proof of delivery.'")
print(f"Estimated Risk: {estimate_risk('The system shall allow customers to upload proof of delivery.', model, tokenizer, risk_categories_mapping)}")
print("-" * 30)
# Actual Risk: 3 (from dataset, RQ-05-14, moderate due to user interaction and data upload)

# Requirement 3: Existing (Smart City, Functional, RQ-08-02)
print(f"Requirement: 'The system shall provide public dashboards for traffic conditions.'")
print(f"Estimated Risk: {estimate_risk('The system shall provide public dashboards for traffic conditions.', model, tokenizer, risk_categories_mapping)}")
print("-" * 30)
# Actual Risk: 1 (from dataset, RQ-08-02, low due to simple public data display)

# Requirement 4: Existing (Education, Functional, RQ-04-05)
print(f"Requirement: 'The system shall allow students to view their grades online.'")
print(f"Estimated Risk: {estimate_risk('The system shall allow students to view their grades online.', model, tokenizer, risk_categories_mapping)}")
print("-" * 30)
# Actual Risk: 3 (from dataset, RQ-04-05, moderate due to sensitive data access)

# Requirement 5: Existing (Agriculture, Non-Functional, RQ-09-07)
print(f"Requirement: 'The system shall encrypt sensitive farm data at rest and in transit.'")
print(f"Estimated Risk: {estimate_risk('The system shall encrypt sensitive farm data at rest and in transit.', model, tokenizer, risk_categories_mapping)}")
print("-" * 30)
# Actual Risk: 5 (from dataset, RQ-09-07, high due to encryption and security)

# Requirement 6: New (Finance, Non-Functional)
print(f"Requirement: 'The system shall ensure transaction processing latency under 1 second.'")
print(f"Estimated Risk: {estimate_risk('The system shall ensure transaction processing latency under 1 second.', model, tokenizer, risk_categories_mapping)}")
print("-" * 30)
# Estimated Risk: 5 (based on similar requirements like RQ-06-15, high due to real-time performance and financial impact)

# Requirement 7: New (Aviation, Functional)
print(f"Requirement: 'The system shall allow pilots to view real-time weather updates during flights.'")
print(f"Estimated Risk: {estimate_risk('The system shall allow pilots to view real-time weather updates during flights.', model, tokenizer, risk_categories_mapping)}")
print("-" * 30)
# Estimated Risk: 4 (based on RQ-13-12, high due to real-time data and safety implications)

# Requirement 8: New (Gaming, Functional)
print(f"Requirement: 'The system shall allow players to customize in-game character appearances.'")
print(f"Estimated Risk: {estimate_risk('The system shall allow players to customize in-game character appearances.', model, tokenizer, risk_categories_mapping)}")
print("-" * 30)
# Estimated Risk: 1 (based on RQ-14-09, low due to simple UI customization)

# Requirement 9: New (Healthcare, Functional)
print(f"Requirement: 'The system shall allow patients to download their lab results in PDF format.'")
print(f"Estimated Risk: {estimate_risk('The system shall allow patients to download their lab results in PDF format.', model, tokenizer, risk_categories_mapping)}")
print("-" * 30)
# Estimated Risk: 3 (based on RQ-01-03, moderate due to sensitive data handling)

# Requirement 10: New (Smart City, Non-Functional)
print(f"Requirement: 'The system shall support secure storage of public transport schedules.'")
print(f"Estimated Risk: {estimate_risk('The system shall support secure storage of public transport schedules.', model, tokenizer, risk_categories_mapping)}")
print("-" * 30)
# Estimated Risk: 4 (based on RQ-13-97, high due to secure storage and public infrastructure)



"""## Save the Trained Model and Tokenizer
Save the trained RoBERTa model and tokenizer to disk so they can be loaded later for inference.
"""

# Save model and tokenizer
model.save_pretrained('./risk_estimation_roberta')
tokenizer.save_pretrained('./risk_estimation_roberta')

print("Model and tokenizer saved successfully to './risk_estimation_roberta'")

"""## Load and Use the Model in FastAPI

To use the saved model in a FastAPI application, you'll need to:

1.  **Install necessary libraries:** Make sure you have `fastapi`, `uvicorn`, `transformers`, `torch`, and `pandas` installed in your environment.
"""

from fastapi import FastAPI
from pydantic import BaseModel
import torch
from transformers import RobertaTokenizer, RobertaForSequenceClassification
import pandas as pd # Import pandas to get the risk_categories_mapping

# Define the categories based on your training data
# Replace this with the actual categories from your training data loading
# It's best to save and load this mapping alongside your model
# For demonstration, assuming you have this mapping available
RISK_CATEGORIES = [1, 2, 3, 4, 5] # Example, replace with your actual categories

# Load the saved model and tokenizer
model_path = './risk_estimation_roberta'
tokenizer = RobertaTokenizer.from_pretrained(model_path)
model = RobertaForSequenceClassification.from_pretrained(model_path)

# Set model to evaluation mode
model.eval()

# Move model to GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

app = FastAPI()

class Requirement(BaseModel):
    text: str

@app.post("/estimate_risk/")
async def estimate_risk_endpoint(requirement: Requirement):
    """
    Estimates the risk level of a given requirement text using the trained RoBERTa model.
    """
    inputs = tokenizer(requirement.text, return_tensors='pt', truncation=True, padding='max_length', max_length=128)

    # Move inputs to the same device as the model
    inputs = {k: v.to(device) for k, v in inputs.items()}

    with torch.no_grad():
        outputs = model(**inputs)

    logits = outputs.logits.cpu()
    pred = torch.argmax(logits, dim=1).item()

    # Map the predicted index back to the original risk level
    # Ensure the index is within the bounds of the categories
    if 0 <= pred < len(RISK_CATEGORIES):
        risk_level = RISK_CATEGORIES[pred]
    else:
        risk_level = "Error: Could not map prediction to a risk level."

    return {"requirement_text": requirement.text, "estimated_risk": risk_level}

# To run this FastAPI application:
# 1. Save the code above as main.py
# 2. Open your terminal, navigate to the directory where you saved main.py
# 3. Run the command: uvicorn main:app --reload
# The API will be running on http://127.0.0.1:8000
# You can then send POST requests to http://127.0.0.1:8000/estimate_risk/